{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423d0c98931afbe",
   "metadata": {},
   "source": [
    "# 2.1. Data Maniputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de92de6e36263c",
   "metadata": {},
   "source": [
    "### Basic Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T11:38:06.592109Z",
     "start_time": "2024-10-30T11:38:06.572552Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  1,  3,  5],\n",
       "        [21,  5,  5,  3],\n",
       "        [ 2,  5,  3,  6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with evenly spaced values, starting at start(included) and ending at n(not included).\n",
    "# By default, the interval length is 1.\n",
    "x = torch.arange(0, 12, dtype=torch.float32)\n",
    "\n",
    "# Use the numel method to calculate the total number of the tensor.\n",
    "tot_num = x.numel()\n",
    "print(tot_num)\n",
    "# Access a tensor's shape by inspecting its shape attribute.\n",
    "x_shape = x.shape\n",
    "\n",
    "# Change the shape of the tensor by invoking reshape.\n",
    "X = x.reshape(3,4)\n",
    "\n",
    "# Put a \"-1\" for the shape component that should be inferred automatically.\n",
    "X_auto = x.reshape(-1,2)\n",
    "\n",
    "# Construct a tensor with all elements set to 0 and shape (shape0, shape1, shape2) by the zeros function.\n",
    "X_0 = torch.zeros((2,3,4,5),dtype=torch.float32)\n",
    "\n",
    "# Construct a tensor with all elements set to 1\n",
    "X_1 = torch.ones((2,3,4))\n",
    "\n",
    "# Create a tensor with random values obeying a standard Gaussian with mean = 0 and standard deviation = 1.\n",
    "X_rand = torch.randn(3,4)\n",
    "\n",
    "# Construct tensors by supplying the exact values for each element.\n",
    "# Where the outermost list corresponds to axis 0 and the inner list corresponds to axis 1.\n",
    "X_construct = torch.tensor([[2,1,3,5],[21,5,5,3],[2,5,3,6]])\n",
    "X_construct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a896ab806046fcc7",
   "metadata": {},
   "source": [
    "### Indexing and Slicing\n",
    "- To access an element based on its position relative to the end of the list, we can use negative indexing. Finally, we can access whole ranges of indices via slicing (e.g., X[start:stop]), where the returned value includes the first index (start) but not the last (stop). \n",
    "- Finally, when only one index (or slice) is specified for a order tensor, it is applied along axis 0. Thus, in the following code, [-1] selects the last row and [1:3] selects the second and third rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c982a67f1d2e9613",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:06:52.676552Z",
     "start_time": "2024-10-30T10:06:52.671615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  9, 10, 11])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.tensor([[0,1,2,3],[4,5,6,7],[8,9,10,11]])\n",
    "\n",
    "X[-1]\n",
    "#X[1:3]\n",
    "\n",
    "#X[1,2]=17\n",
    "#X\n",
    "\n",
    "#X[:2,:] = 12\n",
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0944b76dabdfac1",
   "metadata": {},
   "source": [
    "### Elementwise Operations\n",
    "In mathematical notation, we denote such unary scalar operators (taking one input) by the signature : f:R->R.\n",
    ". This just means that the function maps from any real number onto some other real number. Most standard operators, including unary ones like e^x , can be applied elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98d0e64594b3fcc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:12:07.313980Z",
     "start_time": "2024-10-30T10:12:07.306014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01, 5.4598e+01, 1.4841e+02,\n",
       "        4.0343e+02, 1.0966e+03, 2.9810e+03, 8.1031e+03, 2.2026e+04, 5.9874e+04])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(12,dtype=torch.float32)\n",
    "\n",
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951e02b889b33e2",
   "metadata": {},
   "source": [
    "### Elementwise Binary Operators\n",
    "Here, we produced the vector-valued F:R^d,R^d -> R^d by lifting the scalar function to an elementwise vector operation. The common standard arithmetic operators for addition (+), subtraction (-), multiplication (*), division (/), and exponentiation (**) have all been lifted to elementwise operations for identically-shaped tensors of arbitrary shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7deb5a9d50abf91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:16:32.097265Z",
     "start_time": "2024-10-30T10:16:32.087050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e0ac6dffa750a",
   "metadata": {},
   "source": [
    "### Concatenate multiple tensors\n",
    "- along aixs0 or axis1\n",
    "- eg: (3,4) and (3,4) axis0:(3+3) axis1:(4+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db98c13ae820d3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:19:13.901352Z",
     "start_time": "2024-10-30T10:19:13.893030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250bb04c3ae27ee",
   "metadata": {},
   "source": [
    "### Logical Statement\n",
    "- X == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea71e099bc899c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:21:47.385684Z",
     "start_time": "2024-10-30T10:21:47.381530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "\n",
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b535ef83a18b97",
   "metadata": {},
   "source": [
    "### Sum\n",
    "- .sum(): return a tensor with only one element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73abf12345f0ca16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:23:13.281178Z",
     "start_time": "2024-10-30T10:23:13.276084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbacb4b6749f455c",
   "metadata": {},
   "source": [
    "### Broadcasting \n",
    "By now, you know how to perform elementwise binary operations on two tensors of the same shape. Under certain conditions, even when shapes differ, we can still perform elementwise binary operations by invoking the broadcasting mechanism. Broadcasting works according to the following two-step procedure: (i) expand one or both arrays by copying elements along axes with length 1 so that after this transformation, the two tensors have the same shape; (ii) perform an elementwise operation on the resulting arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6365fe4583f85708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:27:47.681765Z",
     "start_time": "2024-10-30T10:27:47.676128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc635081375016c",
   "metadata": {},
   "source": [
    "### Saving Memory\n",
    "Since the location of the same variable after the calculation, we may use a method to perform the updates in place.\n",
    "##### Using Y[:] = <\"expression\">\n",
    "If the value of X is not reused in subsequent computations, we can also use X[:] = X + Y or X += Y to reduce the memory overhead of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1501d131d22e3664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:41:14.995923Z",
     "start_time": "2024-10-30T10:41:14.989547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Y): 2213544970192\n",
      "id(Z): 2213544973152\n",
      "id(Y): 2213544968512\n",
      "id(Z): 2213544973152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "Z = torch.zeros_like(Y)\n",
    "print('id(Y):',id(Y));\n",
    "print('id(Z):',id(Z))\n",
    "Y = X + Y\n",
    "print('id(Y):',id(Y))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):',id(Z))\n",
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82688b7eadae1ce6",
   "metadata": {},
   "source": [
    "### Conversion to Other Python Objects\n",
    "- Converting to a NumPy tensor (ndarray), or vice versa, is easy. The torch tensor and NumPy array will share their underlying memory, and changing one through an in-place operation will also change the other.\n",
    "- To convert a size-1 tensor to a Python scalar, we can invoke the item function or Python’s built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99f74a61d236fb4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T12:33:29.376497Z",
     "start_time": "2024-10-30T12:33:29.369497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.5000]), 4.5, 4.5, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import torch\n",
    "X = torch.arange(1,4,0.5,dtype=torch.float32)\n",
    "A = X.numpy()\n",
    "B = torch.from_numpy(A)\n",
    "type(A),type(B)\n",
    "\n",
    "a = torch.tensor([4.5])\n",
    "a,a.item(),float(a),int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de106b407c453e8",
   "metadata": {},
   "source": [
    "# 2.2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf79b405ae729b4",
   "metadata": {},
   "source": [
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe338710d3ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39933460f04dc243",
   "metadata": {},
   "source": [
    "# 2.3.Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6848a8df02fed",
   "metadata": {},
   "source": [
    "### Scalars\n",
    "Scalars are implemented as tensors that contain only one element. Below, we assign two scalars and perform the familiar addition, multiplication, division, and exponentiation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04cce0a411ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c586e27e55e6ce",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "Vectors are implemented as \n",
    "-order tensors. In general, such tensors can have arbitrary lengths, subject to memory limitations. Caution: in Python, as in most programming languages, vector indices start at \n",
    ", also known as zero-based indexing, whereas in linear algebra subscripts begin at \n",
    " (one-based indexing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4238fd3e1a141b16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T11:28:01.894414Z",
     "start_time": "2024-10-30T11:28:00.634892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n",
      "3\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(3)\n",
    "print(x[2])\n",
    "print(len(x))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f081e75ef91a4a3",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76407fccdff37a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T11:30:39.172932Z",
     "start_time": "2024-10-30T11:30:39.164999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n",
      "tensor([[ True, False, False],\n",
      "        [False,  True, False],\n",
      "        [False, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "A = torch.arange(9).reshape(3,3)\n",
    "print(A)\n",
    "print(A.T)\n",
    "print(A == A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a105b87ae1905a5",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a55dd3bce16419a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T11:32:37.415444Z",
     "start_time": "2024-10-30T11:32:37.407434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0,  1],\n",
       "          [ 2,  3]],\n",
       "\n",
       "         [[ 4,  5],\n",
       "          [ 6,  7]],\n",
       "\n",
       "         [[ 8,  9],\n",
       "          [10, 11]]],\n",
       "\n",
       "\n",
       "        [[[12, 13],\n",
       "          [14, 15]],\n",
       "\n",
       "         [[16, 17],\n",
       "          [18, 19]],\n",
       "\n",
       "         [[20, 21],\n",
       "          [22, 23]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.arange(24).reshape(2,3,2,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c8b5a0d818e96",
   "metadata": {},
   "source": [
    "### Basic Properties of Tensor Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6bb0fb2ab2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()  # Assign a copy of A to B by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36b410948f81b7",
   "metadata": {},
   "source": [
    "##### Hadamard product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0ad5c3137ecd956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T11:34:32.124056Z",
     "start_time": "2024-10-30T11:34:32.085610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()\n",
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdcd60b4c623902",
   "metadata": {},
   "source": [
    "##### Scalar multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80191b50c35e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f58700ee335baf",
   "metadata": {},
   "source": [
    "##### Matrix-Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2729bec2acdef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cf6a2e2cf05775b",
   "metadata": {},
   "source": [
    "### Reduction Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2e724b519e02c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T11:56:14.582728Z",
     "start_time": "2024-10-30T11:56:14.564461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) tensor(21.)\n",
      "torch.Size([2, 3]) tensor([5., 7., 9.]) torch.Size([3])\n",
      "torch.Size([2, 3]) tensor([ 6., 15.]) torch.Size([2])\n",
      "tensor(True)\n",
      "tensor(3.5000) tensor(3.5000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "A = torch.arange(1,7,1,dtype=torch.float32).reshape(2,3)\n",
    "\n",
    "print(A.shape,A.sum())\n",
    "\n",
    "print(A.shape,A.sum(axis=0),A.sum(axis=0).shape)\n",
    "print(A.shape,A.sum(axis=1),A.sum(axis=1).shape)\n",
    "\n",
    "print(A.sum(axis=[0,1]) == A.sum())\n",
    "\n",
    "print(A.mean(),A.sum() / A.numel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407bbda69320fa4",
   "metadata": {},
   "source": [
    "### Non-Reduction Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc4126e2e954bb92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T12:01:01.137982Z",
     "start_time": "2024-10-30T12:01:01.120900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[ 6.],\n",
      "        [15.]]) torch.Size([2, 1])\n",
      "tensor([[0.1667, 0.3333, 0.5000],\n",
      "        [0.2667, 0.3333, 0.4000]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [5., 7., 9.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "A = torch.arange(1,7,1,dtype=torch.float32).reshape(2,3)\n",
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "print(A)\n",
    "print(sum_A, sum_A.shape)\n",
    "print(A / sum_A)\n",
    "print(A.cumsum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dbc38f613448aa",
   "metadata": {},
   "source": [
    "### Matrix-Vector Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b83205ba65155876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T12:32:32.109264Z",
     "start_time": "2024-10-30T12:32:32.094167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]]) tensor([0., 1., 2.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 5., 14.]), tensor([ 5., 14.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(3,dtype=torch.float32)\n",
    "A = torch.arange(6, dtype=torch.float32).reshape(2,3)\n",
    "print(A,x)\n",
    "torch.mv(A, x),A@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f7c6efff6e9cd",
   "metadata": {},
   "source": [
    "### Norms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66d943c1931b34dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T11:43:40.622847Z",
     "start_time": "2024-10-30T11:43:40.605478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0990)\n",
      "tensor(6.)\n",
      "tensor(36.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([3.0, 4.0, -1.0])\n",
    "y = torch.ones((4,9))\n",
    "print(torch.norm(x))  # Euclidean norm\n",
    "print(torch.norm(y, p=2))  # L2 norm\n",
    "print(torch.norm(y, p=1))  # L1 norm\n",
    "print(torch.norm(y))  #  Frobenius norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee441b1ce07a23c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c36d4349e7c68fd5",
   "metadata": {},
   "source": [
    "# Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ff6b23b510657",
   "metadata": {},
   "source": [
    "### Differentiation and Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "425de3cd267e5d9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T12:38:20.428977Z",
     "start_time": "2024-10-30T12:38:19.365938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000,numerical limit=2.30000\n",
      "h=0.01000,numerical limit=2.03000\n",
      "h=0.00100,numerical limit=2.00300\n",
      "h=0.00010,numerical limit=2.00030\n",
      "h=0.00001,numerical limit=2.00003\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x\n",
    "\n",
    "for h in 10.0**np.arange(-1,-6,-1):\n",
    "    print(f'h={h:.5f},numerical limit={(f(1+h)-f(1))/h:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b929a1f9919bc0ab",
   "metadata": {},
   "source": [
    "### Visualization Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1846e7f9769723b8",
   "metadata": {},
   "source": [
    "We can visualize the slopes of functions using the matplotlib library. We need to define a few functions. As its name indicates, use_svg_display tells matplotlib to output graphics in SVG format for crisper images. The comment #@save is a special modifier that allows us to save any function, class, or other code block to the d2l package so that we can invoke it later without repeating the code, e.g., via d2l.use_svg_display()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3f1e7a0e7e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def use_svg_display():\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
