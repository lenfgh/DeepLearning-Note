{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Linear Regression\n",
    "- As long as the design matrix X has full rank (no feature is linearly dependent on the others), then there will be just one critical point on the loss surface and it corresponds to the minimum of the loss over the entire domain. "
   ],
   "id": "b31afe6aa44d5252"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Minibatch Stochastic Gradient Descent\n",
    "- The specific choice of the size of the said minibatch depends on many factors, such as the amount of memory, the number of accelerators, the choice of layers, and the total dataset size. Despite all that, a number between 32 and 256, preferably a multiple of a large power of 2 , is a good start.\n",
    "- Although the algorithm converges slowly towards the minimizers it typically will not find them exactly in a finite number of steps. Moreover, the minibatches B used for updating the parameters are chosen at random. This breaks determinism.\n",
    "- Linear regression happens to be a learning problem with a global minimum (whenever X\n",
    " is full rank, or equivalently, whenever X^T*X is invertible). \n",
    "##### Vectorization for Speed\n",
    "- When training our models, we typically want to process whole minibatches of examples simultaneously. Doing this efficiently requires that we vectorize the calculations and leverage fast linear algebra libraries rather than writing costly for-loops in Python."
   ],
   "id": "c3df8f0157f2b620"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-03T12:16:49.685672Z",
     "start_time": "2024-11-03T12:16:47.740865Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "n = 10000\n",
    "a = torch.ones(n)\n",
    "b = torch.ones(n)\n",
    "\n",
    "c = torch.zeros(n)\n",
    "t = time.time()\n",
    "for i in range(n):\n",
    "    c[i] = a[i] + b[i]\n",
    "print(f'{time.time()-t:.10f} sec')\n",
    "\n",
    "t = time.time()\n",
    "d = a + b\n",
    "print(f'{time.time()-t:.30f} sec')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0496573448 sec\n",
      "0.000991106033325195312500000000 sec\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Loss Function\n",
    "Naturally, fitting our model to the data requires that we agree on some measure of fitness (or, equivalently, of unfitness)."
   ],
   "id": "531c2e62dd7f418e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Analytic Solution\n",
    "Loss Function for Linear model :\n",
    "$$ L(\\mathbf{w},b) = \\frac{1}{n} \\sum_{i=1}^{n} (\\mathbf{w}^T\\mathbf{x}^{(i)}+b-y^{(i)})^2 $$\n",
    "If we subsume the bias \\mathbf{b} into the parameter \\mathbf{w} by appending a column to the design matrix consisting of all 1s. Then prediction is to minimize  $||\\mathbf{y} - \\mathbf{X}\\mathbf{w}||^2$.\n",
    "As long as the design matrix \\mathbf{X} has full rank(no feature is linearly dependent on the others), then there will be just one critical point on the loss surface and it corresponds to the minimum of the loss over the entire domain.\n",
    "$$ \\partial_\\mathbf{w} ||\\mathbf{y} - \\mathbf{X}\\mathbf{w}||^2 = 2\\mathbf{X^{T}}(\\mathbf{X}\\mathbf{w}-\\mathbf{y})=0\\ and\\ hence\\ \\mathbf{X^{T}\\mathbf{y}=\\mathbf{X^{T}}\\mathbf{X}\\mathbf{w}} $$\n",
    "$$ \\mathbf{w^{*}} = (\\mathbf{X^{T}\\mathbf{X})^{-1}\\mathbf{X^{T}}\\mathbf{y} $$\n",
    "The solution will be unique if the matrix $\\mathbf{X^{T}}\\mathbf{X}\\ $ is invertible.\n",
    "\n"
   ],
   "id": "2d017bc0f37f9b18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### The normal noise\n",
    "$$ y=\\mathbf{w^{T}}\\mathbf{x}+b+\\epsilon\\ where\\ \\epsilon\\ is\\ N(0,\\sigma^{2}) $$\n",
    "The likelihood of seeing a particular y for a given \\mathbf{x} is \n",
    "$$ P(y|\\mathbf{x})={\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} exp(-{\\frac{1}{2\\sigma^{2}}  (y-\\mathbf{w^{T}x}-b)^2) $$\n",
    "According to the principle of maximum likelihood, the best values of parameters $\\mathbf{w}$ and $b$ are those that maximize the likelihood of the entire dataset:\n",
    "$$ P(\\mathbf{y|X})=\\prod_{i=1}^{n} p{y^{i}|x^{i}}$$\n",
    "For historical reasons, optimizations are more often expressed as minimization rather than maximization.\n",
    "So, we can minimize the negative loglikelihood,$$ -logP(\\mathbf{y|X})=\\sum_{i=1}^{n}{\\frac{1}{2}}log(2\\pi\\sigma^{2})+{\\frac{1}{2\\sigma^{2}}}(y^{i}-\\mathbf{w^{T}x^{i}-b})^{2}$$\n",
    "It follows that minimizing the mean squared error is equivalent to the maximum likelihood estimation of a linear model under the assumption of additive Gaussian noise."
   ],
   "id": "5c7fbbcd93568560"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Object-Oriented Design for Implementation\n",
    "+  (i) Module contains models, losses, and optimization methods\n",
    "+  (ii) DataModule provides data loaders for training and validation\n",
    "+  (iii) both classes are combined using the Trainer class"
   ],
   "id": "a720c396275657aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The first utility function allows us to register functions as methods in a class after the class has been created.",
   "id": "421f85adb20d1735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T12:16:49.701673Z",
     "start_time": "2024-11-03T12:16:49.686675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def add_to_class(Class): \n",
    "    \"\"\"Register functions as methods in created class\"\"\"\n",
    "    def wrapper(obj):\n",
    "        setattr(Class, obj.__name__, obj)\n",
    "    return wrapper\n",
    "\n",
    "# example\n",
    "class A:\n",
    "    def __init__(self):\n",
    "        self.b = 1 # initialization of the A\n",
    "\n",
    "\n",
    "@add_to_class(A)\n",
    "def do(self):\n",
    "    print('Class attribute \"b\" is',self.b)\n",
    "\n"
   ],
   "id": "d5a86ee6f97f72fa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The second one is a utility class that saves all arguments in a classâ€™s __init__ method as class attributes. This allows us to extend constructor call signatures implicitly without additional code.",
   "id": "bf9abe5d8c569b80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T12:16:49.717672Z",
     "start_time": "2024-11-03T12:16:49.702672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class HyperParameters: #@save\n",
    "    def save_hyperparameters(self,ignore=[]):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    \n",
    "class B(d2l.HyperParameters):\n",
    "    def __init__(self,a,b,c):\n",
    "        self.save_hyperparameters(ignore=['b'])\n",
    "        print('self.a=',self.a,'self.c=',c)\n",
    "        print('There is no self.b = ', not hasattr(self,'b'))\n",
    "\n",
    "b = B(a=1,b=2,c=3)"
   ],
   "id": "ecabbb194491c498",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.a= 1 self.c= 3\n",
      "There is no self.b =  True\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The final utility allows us to plot experiment progress interactively while it is going on. In deference to the much more powerful (and complex) TensorBoard we name it ProgressBoard. The implementation is deferred to Section 23.7.",
   "id": "121ed4f62542213f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-03T12:16:49.718939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class HyperParameters: \n",
    "    def save_hyperparameters(self,ignore=[]):\n",
    "        raise NotImplemented\n",
    "\n",
    "class ProgressBoard(d2l.HyperParameters):\n",
    "    \"\"\"The Board that plots data points in animation\"\"\"\n",
    "    def __init__(self,xlabel=None,ylabel=None,xlim=None,ylim=None,\n",
    "                 xscale='linear',yscale='linear',ls=['-','--','-.',':'],colors=['C0','C1','C2','C3'],fig=None,axes=None,figsize=(3.5,2.5),display=True):\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def draw(self,x,y,label,every_n=1):\n",
    "        raise NotImplemented\n",
    "\n",
    "board = d2l.ProgressBoard('y')\n",
    "for x in np.arange(0,10,0.1):\n",
    "    board.draw(x,np.sin(x),'sin',every_n=2)\n",
    "    board.draw(x,np.cos(x),'cos',every_n=10)"
   ],
   "id": "7b8d0f41efa89bf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Module",
   "id": "97140742ef42e66b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class Module(nn.Module,d2l.HyperParameters):\n",
    "    \"\"\"The base class of models\"\"\"\n",
    "    def __init__(self,plot_train_per_epoch=2, plot_valid_per_epoch=1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.board = ProgressBoard()\n",
    "        \n",
    "    def loss(self,y_hat, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self,X):\n",
    "        assert hasattr(self, 'net'),'Neural network is defined'\n",
    "        return self.net(X)\n",
    "    \n",
    "    def plot(self,key, value, train):\n",
    "        \"\"\"Plot a point in animation\"\"\"\n",
    "        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n",
    "        self.board.xlabel = 'epoch'\n",
    "        if train:\n",
    "            x = self.trainer.train_batch_idx / \\\n",
    "                self.trainer.num_train_batches\n",
    "            n = self.trainer.num_train_batcher / \\\n",
    "                self.plot_train_per_epoch\n",
    "        else:\n",
    "            x = self.trainer.epoch + 1\n",
    "            n = self.trainer.num_val_batches / \\ \n",
    "                self.plot_valid_per_epoch\n",
    "        self.board.draw(x, value.to(d2l.cpu()).detach().numpy(),('train_' if train else 'val_') + key,every_n=int(n))\n",
    "        \n",
    "    def training_step(self,batch):\n",
    "        l = self.loss(self(*batch[:-1]),batch[-1])\n",
    "        self.plot('loss', l, train=True)\n",
    "        \n",
    "    def validation_step(self,batch):\n",
    "        l = self.loss(self(*batch[:-1]),batch[-1])\n",
    "        self.plot('loss', l, train=False)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        raise NotImplementedError\n",
    "        "
   ],
   "id": "c093a2e1457b6772",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### DataModule",
   "id": "7383cba92d03badf"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class DataModule(d2l.HyperParameters):\n",
    "    \"\"\"The base of the data\"\"\"\n",
    "    def __init__(self,root='../data',num_workers=4):\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def get_dataloader(self, train):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(train=False)"
   ],
   "id": "e8971bbc5bb3e54c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Training",
   "id": "6abcb630910bbf30"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer(d2l.HyperParameters):\n",
    "    \"\"\"THe base class for training models with data\"\"\"\n",
    "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "        \n",
    "        "
   ],
   "id": "3d5c9f53c8c2b15a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Synthetic Regression Data\n",
    "##### Reading the Dataset"
   ],
   "id": "65811d202870692f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class DataModule(d2l.HyperParameters):\n",
    "    \"\"\"The base of the data\"\"\"\n",
    "    def __init__(self,root='../data',num_workers=4):\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def get_dataloader(self, train):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(train=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(train=False)\n",
    "# \n",
    "class SyntheticRegressionData(d2l.DataModule): #@save\n",
    "    \"\"\"Synthetic data for linear regression\"\"\"\n",
    "    def __init__(self,w,b,noise=0.01,num_train=1000,num_val=1000,batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n,len(w))\n",
    "        noise = torch.randn(n,1) * noise\n",
    "        self.y = torch.matmul(self.X,w.reshape((-1,1))) + b + noise\n",
    "\"\"\"\n",
    "@d2l.add_to_class(SyntheticRegressionData)\n",
    "def get_dataloader(self, train):\n",
    "    if train:\n",
    "        indices = list(range(0, self.num_train))\n",
    "        # The examples are read in random order\n",
    "        random.shuffle(indices)\n",
    "    else:\n",
    "        indices = list(range(self.num_train, self.num_train+self.num_val))\n",
    "    for i in range(0, len(indices), self.batch_size):\n",
    "        batch_indices = torch.tensor(indices[i: i+self.batch_size])\n",
    "        yield self.X[batch_indices], self.y[batch_indices]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=torch.tensor([4.2]))\n",
    "\n",
    "@d2l.add_to_class(DataModule)\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0,None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size, shuffle=train)\n",
    "\n",
    "@d2l.add_to_class(SyntheticRegressionData)\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "\n",
    "\n",
    "X,y = next(iter(data.train_dataloader()))\n",
    "\n",
    "print(X.shape,y.shape)"
   ],
   "id": "831d278e017700a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Regression Implementation from Scratch",
   "id": "77df7e57a033e873"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T12:25:00.863985Z",
     "start_time": "2024-11-03T12:25:00.814153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class SyntheticRegressionData(d2l.DataModule): \n",
    "    \"\"\"Synthetic data for linear regression\"\"\"\n",
    "    def __init__(self,w,b,noise=0.01,num_train=1000,num_val=1000,batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n,len(w))\n",
    "        noise = torch.randn(n,1) * noise\n",
    "        self.y = torch.matmul(self.X,w.reshape((-1,1))) + b + noise\n",
    "\n",
    "@d2l.add_to_class(SyntheticRegressionData)\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "    return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "class LinearRegressionScratch(d2l.Module):\n",
    "    \"\"\"the linear regression model implemented from scratch\"\"\"\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1,requires_grad=True)\n",
    "        \n",
    "@d2l.add_to_class(LinearRegressionScratch) \n",
    "def forward(self,X):\n",
    "    return torch.matmul(X,self.w) + self.b\n",
    "\n",
    "@d2l.add_to_class(LinearRegressionScratch)\n",
    "def loss(self, y_hat, y):\n",
    "    l = 1/2*(y_hat-y)**2\n",
    "    return l.mean()\n",
    "\n",
    "class Trainer(d2l.HyperParameters): \n",
    "    \"\"\"The base class for training models with data.\"\"\"\n",
    "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "        self.save_hyperparameters()\n",
    "        assert num_gpus == 0, 'No GPU support yet'\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloader()\n",
    "        self.num_train_batches = len(self.train_dataloader)\n",
    "        self.num_val_batches = (len(self.val_dataloader)\n",
    "                                if self.val_dataloader is not None else 0)\n",
    "\n",
    "    def prepare_model(self, model):\n",
    "        model.trainer = self\n",
    "        model.board.xlim = [0, self.max_epochs]\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, model, data):\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim = model.configure_optimizers()\n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()\n",
    "\n",
    "    def fit_epoch(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(d2l.HyperParameters):\n",
    "    \"\"\"Minibatch stochastic gradient descent\"\"\"\n",
    "    def __init__(self, params, lr):\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "@d2l.add_to_class(LinearRegressionScratch)\n",
    "def configure_optimizers(self):\n",
    "    return SGD([self.w, self.b], self.lr)\n",
    "\n",
    "model1 = LinearRegressionScratch(2, lr=0.03)\n",
    "data1 = SyntheticRegressionData(w=torch.tensor([2,-3.4]),b=4.2)\n",
    "trainer = Trainer(max_epochs=3)\n",
    "trainer.fit(model1,data1)\n",
    "\n",
    "print(model1.w.reshape(data.w.reshape))\n",
    "print(model1.b)"
   ],
   "id": "96992b3ec9f901f3",
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 88\u001B[0m\n\u001B[0;32m     86\u001B[0m data1 \u001B[38;5;241m=\u001B[39m SyntheticRegressionData(w\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3.4\u001B[39m]),b\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4.2\u001B[39m)\n\u001B[0;32m     87\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m---> 88\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel1\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdata1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28mprint\u001B[39m(model1\u001B[38;5;241m.\u001B[39mw\u001B[38;5;241m.\u001B[39mreshape(data\u001B[38;5;241m.\u001B[39mw\u001B[38;5;241m.\u001B[39mreshape))\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28mprint\u001B[39m(model1\u001B[38;5;241m.\u001B[39mb)\n",
      "Cell \u001B[1;32mIn[3], line 62\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, data)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval_batch_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_epochs):\n\u001B[1;32m---> 62\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[3], line 65\u001B[0m, in \u001B[0;36mTrainer.fit_epoch\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_epoch\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a7d33dc9a13886f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "    "
   ],
   "id": "ea89246f4df5abc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
